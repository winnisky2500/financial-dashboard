# -*- coding: utf-8 -*-
from __future__ import annotations
import os, json, time, math, re
from typing import Any, Dict, List, Optional
from enum import Enum

import requests
from fastapi import FastAPI, HTTPException, Depends, Header, Request
from starlette.responses import JSONResponse, Response
from fastapi.responses import StreamingResponse        # â† æ–°å¢
import asyncio                                         # â† æ–°å¢

from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from dotenv import load_dotenv, find_dotenv
import json, re
from math import isfinite

# æ›´é²æ£’ï¼šæ— è®ºåœ¨ä»€ä¹ˆå·¥ä½œç›®å½•å¯åŠ¨ï¼Œéƒ½èƒ½æ‰¾åˆ°.env.local
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any
import datetime as _dt
import requests




# --- åœ¨æ–‡ä»¶å¤´éƒ¨ imports ä¹‹åï¼Œç¡®ä¿é¡ºåºå¦‚ä¸‹ï¼ˆæ›¿æ¢åŸæœ‰ env è¯»å–ç‰‡æ®µï¼‰ ---
from dotenv import load_dotenv, find_dotenv
p = find_dotenv(".env.backend", raise_error_if_not_found=False)
load_dotenv(p, override=True)

LLM_BASE  = (os.getenv("OPENAI_BASE_URL") or os.getenv("OPENAI_API_BASE") or os.getenv("LLM_BASE_URL") or "").rstrip("/")
LLM_KEY   = os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY") or ""
LLM_MODEL = os.getenv("OPENAI_MODEL") or os.getenv("LLM_MODEL") or ""
LLM_CONNECT_TIMEOUT = int(os.getenv("LLM_CONNECT_TIMEOUT") or 30)   # åŸæ¥ 5
LLM_READ_TIMEOUT    = int(os.getenv("LLM_READ_TIMEOUT")    or 90)   # åŸæ¥ 20
# æ˜¯å¦åœ¨ç»´åº¦ä¸‹é’»é‡Œéšè—å®Œæ•´è¡¨ï¼Œä»…è¾“å‡º TOP è¡¨ï¼ˆé»˜è®¤æ˜¯ï¼‰
COMPACT_DIMENSION_TABLES = (os.getenv("COMPACT_DIMENSION_TABLES") or "true").lower() == "true"
# === Google CSE for policy/news ===
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "").strip()
GOOGLE_CSE_ID  = os.getenv("GOOGLE_CSE_ID", "").strip()


print("[deepanalysis LLM]", LLM_BASE, LLM_MODEL)  # å¯åŠ¨æ—¥å¿—æ˜ç¡®å½“å‰é…ç½®

# å¼€å‘æœŸæ˜¯å¦è·³è¿‡é‰´æƒ
DEV_BYPASS_AUTH = (os.getenv("DEV_BYPASS_AUTH") or "true").lower() == "true"
# æ€è€ƒç»“æŸ0.5ç§’åå†ç”Ÿæˆç»“æœ
THOUGHT_DELAY_MS = int(os.getenv("THOUGHT_DELAY_MS") or "600")  # æœ€å°‘ 0.6s
# ä¸‹æ¸¸ dataquery_agent
DATA_AGENT_BASE_URL = (
    os.getenv("DATA_AGENT_BASE_URL") or os.getenv("DATA_API") or "http://127.0.0.1:18010"
)
DATA_AGENT_TOKEN = os.getenv("DATA_AGENT_TOKEN") or os.getenv("ROE_AGENT_TOKEN") or ""

# Supabase RESTï¼ˆè¯»è¡¨ï¼‰
SUPABASE_URL = (
    os.getenv("SUPABASE_URL")
    or os.getenv("NEXT_PUBLIC_SUPABASE_URL")  # å‰ç«¯å…¬é’¥ä¹Ÿå¯ç”¨
)
SUPABASE_SERVICE_ROLE_KEY = (
    os.getenv("SUPABASE_SERVICE_ROLE_KEY")
    or os.getenv("SUPABASE_ANON_KEY")         # åªè¯»åœºæ™¯å¯é€€åŒ–ä¸º anon
)
if not (SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY):
    raise RuntimeError("ç¼ºå°‘ SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY ç¯å¢ƒå˜é‡")

# ======= å¯è°ƒæç¤ºè¯ï¼ˆå¯åœ¨ .env.local ç”¨åŒåå˜é‡è¦†ç›–ï¼‰ =======
PROMPT_PLANNER = os.getenv("PROMPT_PLANNER", """
ä½ æ˜¯èµ„æ·±è´¢åŠ¡åˆ†æè§„åˆ’å¸ˆã€‚åŸºäºå·²è§£æçš„ä¸Šä¸‹æ–‡ï¼ˆå…¬å¸/æŒ‡æ ‡/æœŸé—´/å·²é€‰ä¸‹é’»æ¨¡å¼ï¼‰ï¼Œ
ç”¨5æ¡ä»¥å†…ä¸­æ–‡è¦ç‚¹åˆ—å‡ºâ€œåˆ†ææµç¨‹è®¡åˆ’â€ï¼ˆä¸è¦å±•å¼€æ‰§è¡Œï¼‰ï¼š
1) å–æ•°è®¡åˆ’ï¼ˆè¦å–å“ªäº›æ ¸å¿ƒå­—æ®µï¼‰ï¼›
2) å…¬å¼/å…¬å¸/å­å…¬å¸åŒ¹é…è®¡åˆ’ï¼ˆè‹¥ä¸ºæŒ‡æ ‡/ä¸šåŠ¡åˆ†è§£è¦æŒ‡æ˜å…¬å¼å/å˜é‡åï¼‰ï¼›
3) ä¸‹é’»åˆ†è§£è®¡åˆ’ï¼ˆåŒæ¯”/ç¯æ¯”ã€ç»´åº¦æ‹†è§£ç­‰ï¼‰ï¼›
4) ä¸Šä¸‹æ–‡æ”¿ç­–è®¡åˆ’ï¼ˆè¦å…³æ³¨å“ªäº›æ”¿ç­–è„‰ç»œ/å£å¾„ï¼‰ï¼›
5) ç»“è®ºä¸å»ºè®®çš„é¢„æœŸç»“æ„ã€‚
ä»…è¾“å‡ºç®€çŸ­è¦ç‚¹ï¼Œä¸è¦å¤šä½™è§£é‡Šã€‚
""").strip()

PROMPT_ANALYST = os.getenv("PROMPT_ANALYST", """
ä½ æ˜¯èµ„æ·±è´¢åŠ¡åˆ†æå¸ˆã€‚è¾“å…¥ï¼š
- indicator_cardï¼šå«æœ€æ–°å€¼/åŒæ¯”/ç¯æ¯”/ç›®æ ‡å·®è·
- resolvedï¼šå…¬å¸/æŒ‡æ ‡/æœŸé—´/æ¨¡å¼
- sectionsï¼šæœ¬æ¬¡æ‰€æœ‰å­ä»»åŠ¡çš„ç»“æœï¼ˆç»´åº¦/ä¸šåŠ¡/å¼‚åŠ¨/æ”¿ç­–ç­‰ï¼‰

è¯·å…ˆ**è¯»å–æ‰€æœ‰ sections** ä¸ indicator_cardï¼Œå†ç»™å‡ºé«˜ç®¡å¯è¯»çš„**ä¸€æ¬¡æ€§æœ€ç»ˆè¾“å‡º**ï¼Œä»…è¿”å› JSONï¼š
{
  "summary": "1) **æŒ‡æ ‡æ•´ä½“æè¿°**ï¼šâ€¦\\n2) **ä¸‹é’»è¦ç‚¹**ï¼ˆåˆå¹¶ç»´åº¦/ä¸šåŠ¡ï¼‰ï¼šâ€¦\\n3) **é«˜è´¡çŒ®é¡¹**ï¼šâ€¦ï¼›**å¼‚å¸¸é¡¹**ï¼šâ€¦\\n4) **æ”¿ç­–å½±å“ï¼ˆä»…ä¸€æ¬¡ï¼‰**ï¼šâ€¦\\n5) **é£é™©**ï¼šâ€¦ï¼›**å»ºè®®æ–¹å‘**ï¼šâ€¦",
  "extra_sections": [
    {"title": "ä¸šåŠ¡æ‹†è§£", "message": "1) æœ¬æœŸè´¡çŒ®â€¦\\n2) åŒæ¯”/ç¯æ¯”å·®å¼‚â€¦"},
    {"title": "å¼‚åŠ¨ä¸å½’å› ", "message": "â€¦"}
  ]
}
ç¡¬æ€§è¦æ±‚ï¼š
- **å¿…é¡»**ç»¼åˆæ‰€æœ‰å­ä»»åŠ¡å†ä¸‹ç»“è®ºï¼›ä¸è¦è¾“å‡ºæ€è€ƒ/æ­¥éª¤ï¼›ä¸è¦é‡å¤ç»˜å›¾ï¼›ä¿¡æ¯ä¸è¶³ä¹Ÿç»™å‡ºé€šç”¨æ¡†æ¶ã€‚
""").strip()




PROMPT_POLICY = os.getenv("PROMPT_POLICY", """
ä½ æ˜¯ä¼ä¸šæ”¿ç­–å½±å“åˆ†æå¸ˆã€‚è¾“å…¥ç»™ä½ ï¼š
- resolvedï¼ˆå…¬å¸/æŒ‡æ ‡/æœŸé—´ï¼‰
- sectionsï¼ˆå·²æ‰§è¡Œçš„ä¸‹é’»ç»“æœï¼‰
- policy_newsï¼ˆå¦‚æœ‰ï¼šæ¥è‡ª Google CSE çš„æ”¿ç­–/ç›‘ç®¡æœç´¢å‘½ä¸­ï¼Œæ•°ç»„ï¼Œæ¯é¡¹å« title/link/snippet/source/dateï¼‰

è¯·ç»“åˆ policy_newsï¼ˆè‹¥å­˜åœ¨ï¼‰ä¸å·²çŸ¥ä¸Šä¸‹æ–‡ï¼Œäº§å‡ºä¸è¯¥æŒ‡æ ‡ç›¸å…³çš„â€œæ”¿ç­–ä¸Šä¸‹æ–‡â€åŠå¯èƒ½çš„å½±å“è·¯å¾„ï¼ŒJSON è¿”å›ï¼š
{
  "title": "æ”¿ç­–ä¸Šä¸‹æ–‡",
  "message": "ä¸­æ–‡æ®µè½ï¼Œè¦†ç›–æ”¿ç­–åç§°/çº§åˆ«æˆ–å…³é”®æ¡ç›®ã€å£å¾„å·®å¼‚ä¸å½±å“æœºåˆ¶ï¼›å¯å¼•ç”¨policy_newsçš„å…³é”®ä¿¡æ¯ï¼ˆä¸éœ€è¦ç²˜è´´é“¾æ¥æœ¬èº«ï¼‰",
  "table": [{"policy":"æ”¿ç­–è¦ç‚¹","impact":"å¯èƒ½å½±å“è·¯å¾„","risk":"é£é™©ç‚¹/æ³¨æ„äº‹é¡¹"}]
}
ä»…è¿”å› JSONï¼Œä¸è¦å¤šä½™æ–‡æœ¬ï¼›å¦‚ä¿¡æ¯ä¸è¶³ï¼Œè¯·ç»™å‡ºåˆç†çš„é€šç”¨æ¡†æ¶ã€‚
""").strip()

def _extract_json_block(s: str) -> Optional[dict]:
    """ä» LLM æ–‡æœ¬é‡Œæå– JSONï¼ˆå®¹å¿è¢«```json åŒ…è£¹ï¼‰"""
    if not s: return None
    import re, json as _json
    m = re.search(r"\{[\s\S]*\}$", s.strip())
    if not m:
        m = re.search(r"```json([\s\S]*?)```", s.strip(), re.I)
        if m: s = m.group(1)
    try: return _json.loads(s)
    except Exception: return None

def _down_headers(token: Optional[str]) -> dict:
    h = {"Content-Type": "application/json"}
    t = (token or "").strip()
    if t.lower().startswith("bearer "):
        h["Authorization"] = t
    elif t:
        h["Authorization"] = f"Bearer {t}"
    return h

def llm_chat(system: str, user: str, *, temperature: float = 0.3, want_json: bool = False):
    """
    é€‚é… gpt-5/o4/o3ï¼š/responses + inputï¼Œä¸å‘é€ temperatureï¼›
    å…¶å®ƒï¼š/chat/completions + messages å¯å¸¦ temperatureã€‚
    è¿”å›ï¼šwant_json=True æ—¶å°è¯•æå– JSONï¼Œå¦åˆ™è¿”å›çº¯æ–‡æœ¬ï¼ˆNone è¡¨ç¤ºå¤±è´¥ï¼‰ã€‚
    """
    if not (LLM_BASE and LLM_KEY and LLM_MODEL):
        return {} if want_json else None

    is_responses = str(LLM_MODEL).lower().startswith(("gpt-5", "o4", "o3"))
    endpoint = "/responses" if is_responses else "/chat/completions"
    url = f"{LLM_BASE}{endpoint}"

    if is_responses:
        payload = {
            "model": LLM_MODEL,
            "input": [
                {"role": "system", "content": system},
                {"role": "user",   "content": user},
            ]
        }
    else:
        payload = {
            "model": LLM_MODEL,
            "messages": [
                {"role": "system", "content": system},
                {"role": "user",   "content": user},
            ],
            "temperature": temperature,
        }

    try:
        r = requests.post(
            url,
            headers={"Authorization": f"Bearer {LLM_KEY}", "Content-Type": "application/json"},
            json=payload,
            timeout=(LLM_CONNECT_TIMEOUT, LLM_READ_TIMEOUT),
        )
        if not r.ok:
            # è¿”å›æ›´å¯è¯»çš„é”™è¯¯ï¼Œæ–¹ä¾¿ä½ åœ¨è¿›åº¦é¢æ¿çœ‹åˆ°çœŸæ­£åŸå› 
            try:
                err = r.json()
            except Exception:
                err = {"text": r.text}
            return {} if want_json else None

        data = r.json()
        # â€”â€” è§£ææ–‡æœ¬ï¼ˆå¤šç§è¿”å›å½¢æ€ï¼‰ â€”â€” #
        text = None
        if isinstance(data, dict) and "output_text" in data:  # responses ç›´å‡º
            text = data.get("output_text")
        if text is None and isinstance(data, dict) and "choices" in data and data["choices"]:
            ch0 = data["choices"][0]
            text = (ch0.get("message") or {}).get("content") or ch0.get("text")
        if text is None and isinstance(data, dict) and "output" in data and data["output"]:
            try:
                parts = data["output"][0].get("content", [])
                texts = [p.get("text") for p in parts if isinstance(p, dict) and p.get("text")]
                text = "\n".join(texts) if texts else None
            except Exception:
                pass

        if want_json:
            return _extract_json_block(text or "") or {}
        return (text or "").strip() or None
    except Exception:
        return {} if want_json else None


def _quarter_bounds(year: int, q: str) -> tuple[str, str]:
    qn = int(str(q).upper().replace("Q",""))
    start_month = (qn-1)*3 + 1
    end_month   = start_month + 2
    start = _dt.date(year, start_month, 1)
    # å–è¯¥å­£åº¦æœ€åä¸€å¤©ï¼ˆä¸‹å­£åº¦ç¬¬ä¸€å¤©-1ï¼‰
    if end_month == 12:
        end = _dt.date(year, 12, 31)
    else:
        end = _dt.date(year, end_month+1, 1) - _dt.timedelta(days=1)
    return (start.isoformat(), end.isoformat())

def _google_cse_policy_search(company_name: str|None, industry: str|None,
                              year: int, quarter: str, limit: int = 6) -> list[dict]:
    """
    ç”¨ Google CSE æœç´¢è¯¥å­£åº¦å†…ä¸è¡Œä¸š/å…¬å¸ç›¸å…³çš„æ”¿ç­–ä¸ç›‘ç®¡/å£å¾„åŠ¨æ€ã€‚
    é‡‡ç”¨ã€æ­£å‘çº¦æŸã€‘ï¼šæƒå¨åŸŸåç™½åå• + æ”¿ç­–/é‡‘è/ç›‘ç®¡ç­‰å£å¾„è¯å¿…å«ï¼Œé¿å…æ— å…³ç»“æœã€‚
    è¿”å›ï¼š[{title, link, snippet, source, date}]
    """
    if not (GOOGLE_API_KEY and GOOGLE_CSE_ID):
        return []
    qs, qe = _quarter_bounds(year, quarter)

    keys = [k for k in [industry, company_name] if k]

    extra = ""
    ind = (industry or "")
    if any(x in ind for x in ["æ¸¯", "ç å¤´", "èˆªè¿", "é›†è£…ç®±"]):
        extra = " (æ¸¯å£ OR èˆªè¿ OR é›†è£…ç®± OR å£å²¸ OR é€šå…³ OR è´§è¿)"
    elif "é‡‘è" in ind:
        extra = " (é‡‘è OR é“¶è¡Œ OR ä¿é™© OR è¯åˆ¸ OR è´·æ¬¾ OR èèµ„)"
    elif ("åœ°äº§" in ind) or ("æˆ¿åœ°äº§" in ind):
        extra = " (æˆ¿åœ°äº§ OR åœŸåœ° OR é¢„å”® OR ä½å»º OR èèµ„ç›‘ç®¡)"

    kw = "(æ”¿ç­– OR é€šçŸ¥ OR æŒ‡å¼• OR æ„è§ OR åŠæ³• OR ç›‘ç®¡ OR å®è§‚ OR è´§å¸æ”¿ç­– OR ç¨ OR è´¢æ”¿ OR å›½èµ„ OR å‘æ”¹)"
    base = " ".join(keys) + f" {kw}{extra} {year}å¹´"

    params = {
        "key": GOOGLE_API_KEY,
        "cx":  GOOGLE_CSE_ID,
        "q":   base,
        "num": min(max(limit,1),10),
        "sort": "date",
    }

    # æƒå¨åŸŸåç™½åå•ï¼ˆåªä¿ç•™è¿™äº›æˆ–å…¶å­åŸŸï¼‰
    white_domains = [
        "gov.cn", "ndrc.gov.cn", "mof.gov.cn", "pbc.gov.cn", "csrc.gov.cn",
        "cbirc.gov.cn", "safe.gov.cn", "sasac.gov.cn", "stats.gov.cn",
        "mot.gov.cn", "customs.gov.cn", "sse.com.cn", "szse.cn",
        "people.com.cn", "xinhuanet.com", "ce.cn", "china.com.cn"
    ]
    def domain_ok(src: str) -> bool:
        return any(src.endswith(d) or (("." + d) in src) for d in white_domains)

    # æ”¿ç­–/é‡‘è/ç›‘ç®¡å£å¾„å¿…å«ï¼ˆæ ‡é¢˜+æ‘˜è¦ï¼‰
    must_tokens = ["æ”¿ç­–","é€šçŸ¥","æ„è§","åŠæ³•","ç›‘ç®¡","å®è§‚","è´§å¸","è´¢æ”¿","ç¨","å›½èµ„","å‘æ”¹","é“¶è¡Œ","è¯åˆ¸","ä¿é™©","æ¸¯å£","èˆªè¿","ç‰©æµ","å£å²¸","é€šå…³","èèµ„","è´·æ¬¾","ä½å»º","åœŸåœ°"]

    try:
        r = requests.get("https://www.googleapis.com/customsearch/v1", params=params, timeout=15)
        r.raise_for_status()
        items = r.json().get("items", []) or []
        out = []
        for it in items:
            title = it.get("title") or ""
            snip  = it.get("snippet") or ""
            src   = (it.get("displayLink") or "").lower()
            text  = (title + " " + snip)
            if not domain_ok(src):           # 1) æƒå¨åŸŸå
                continue
            if not any(tok in text for tok in must_tokens):  # 2) å£å¾„è¯
                continue
            out.append({
                "title":   title,
                "link":    it.get("link"),
                "snippet": snip,
                "source":  src,
                "date":    None
            })
        return out
    except Exception:
        return []

    
def _sb_headers():
    return {
        "apikey": SUPABASE_SERVICE_ROLE_KEY,
        "Authorization": f"Bearer {SUPABASE_SERVICE_ROLE_KEY}",
        "Content-Type": "application/json"
    }

import requests as _rq
def _sb_select(table: str, params: Dict[str, Any]) -> List[Dict[str, Any]]:
    url = f"{SUPABASE_URL}/rest/v1/{table}"
    r = _rq.get(url, headers=_sb_headers(), params={"select": "*", **params}, timeout=20)
    if r.status_code >= 400:
        raise HTTPException(r.status_code, r.text)
    return r.json()

def _norm(s: Optional[str]) -> str:
    import re as _re
    return _re.sub(r"\s+", "", (s or "").lower())

def _split_aliases(v: Any) -> List[str]:
    if v is None: return []
    if isinstance(v, list): return [str(x) for x in v]
    sv = str(v).strip()
    if sv.startswith("["):
        try: return [str(x) for x in json.loads(sv)]
        except Exception: pass
    import re as _re
    return [x.strip() for x in _re.split(r"[,\|/;ï¼›ï¼Œã€\s]+", sv) if x.strip()]
# === æ˜ å°„ï¼šcompute_key <-> canonical_nameï¼ˆä¸­æ–‡ï¼‰ ===
def load_alias_maps(conn):
    rows = conn.execute("""
        select compute_key, canonical_name 
        from metric_alias_catalog
        where compute_key is not null and canonical_name is not null
    """).fetchall()
    key2cn = {r["compute_key"].strip(): r["canonical_name"].strip() for r in rows}
    cn2key = {v: k for k, v in key2cn.items()}
    return key2cn, cn2key

VAR_WORD = re.compile(r"\b[a-zA-Z_]\w*\b")

def evaluate_formula(conn, company, year, quarter, variables_json, compute_json):
    # è§£æ
    variables = json.loads(variables_json or "{}")
    compute = json.loads(compute_json or "{}")
    expr = next(iter(compute.values()), "")

    if not expr:
        return {"ok": False, "reason": "å…¬å¼ä¸ºç©º"}

    # è¯»æ˜ å°„
    key2cn, _ = load_alias_maps(conn)

    # è‡ªåŠ¨è¡¥å…¨ variablesï¼šå‡¡æ˜¯è¡¨è¾¾å¼é‡Œå‡ºç°ã€variables åˆæ²¡æœ‰çš„è®¡ç®—é”®ï¼Œè¡¥æˆä¸­æ–‡å
    for k in set(VAR_WORD.findall(expr)):
        if k not in variables and k in key2cn:
            variables[k] = key2cn[k]

    if not variables:
        return {"ok": False, "reason": "å…¬å¼ç¼ºå°‘å˜é‡æ˜ å°„"}

    # æŒ‰ä¸­æ–‡åå»äº‹å®è¡¨å–å€¼
    base_cn = [v for v in variables.values() if v]
    q = """
      select metric_name, metric_value
      from financial_metrics
      where company_name = ? and year = ? and quarter = ?
        and metric_name = any(?)
    """
    rows = conn.execute(q, [company, int(year), int(quarter), base_cn]).fetchall()
    name2val = {r["metric_name"]: float(r["metric_value"]) for r in rows if r["metric_value"] is not None}

    # è®¡ç®—é”® -> æ•°å€¼
    key2val = {}
    missing = []
    for k, cn in variables.items():
        if cn in name2val:
            key2val[k] = name2val[cn]
        else:
            missing.append(cn)

    if missing:
        return {"ok": False, "reason": "åŸºç¡€æŒ‡æ ‡ç¼ºå¤±: " + "ï¼Œ".join(missing)}

    # ä»£å…¥è¡¨è¾¾å¼
    substituted = expr
    for k, v in key2val.items():
        substituted = re.sub(rf"\b{k}\b", f"({v})", substituted)

    if VAR_WORD.search(substituted):  # ä»æœ‰æœªæ›¿æ¢å˜é‡
        return {"ok": False, "reason": "å­˜åœ¨æœªæ›¿æ¢å˜é‡", "substituted": substituted}

    try:
        result = eval(substituted, {"__builtins__": {}})
        if not (isinstance(result, (int, float)) and isfinite(result)):
            return {"ok": False, "reason": "ç»“æœéæ•°å€¼", "substituted": substituted}
        return {"ok": True, "result": float(result), "substituted": substituted, "variables_cn": list(variables.values())}
    except Exception as e:
        return {"ok": False, "reason": f"è®¡ç®—å¼‚å¸¸: {e}", "substituted": substituted}
    
_CACHE_TTL = 60
_LAST_LOAD = 0.0
_COMPANIES: List[Dict[str, Any]] = []
_METRIC_ALIASES: List[Dict[str, Any]] = []
_FORMULAS: List[Dict[str, Any]] = []
_KEY2CANON: Dict[str, str] = {}
_ALIAS2CANON: Dict[str, str] = {}

def _rebuild_alias_maps():
    global _KEY2CANON, _ALIAS2CANON
    _KEY2CANON, _ALIAS2CANON = {}, {}
    for r in _METRIC_ALIASES:
        canon = (r.get("canonical_name") or "").strip()
        if not canon: continue
        ck = (r.get("compute_key") or "").strip()
        if ck: _KEY2CANON[ck] = canon
        for a in _split_aliases(r.get("aliases")) + [r.get("display_name_cn") or "", canon]:
            a = str(a).strip()
            if a: _ALIAS2CANON[a] = canon

def _reload_caches(force=False):
    global _LAST_LOAD, _COMPANIES, _METRIC_ALIASES, _FORMULAS
    now = time.time()
    if (now - _LAST_LOAD < _CACHE_TTL) and not force and _COMPANIES: return
    _COMPANIES = _sb_select("company_catalog", {"order": "id.asc"})
    _METRIC_ALIASES = _sb_select("metric_alias_catalog", {"order": "id.asc"})
    _FORMULAS = _sb_select("metric_formulas", {"order": "id.asc"})
    _rebuild_alias_maps()
    _LAST_LOAD = now

def fmt_num(v: Any) -> Optional[str]:
    try:
        if v is None: return None
        x = float(v)
        if math.isnan(x) or math.isinf(x): return None
        ax = abs(x)
        if ax > 10000: return f"{int(round(x)):,.0f}"
        if ax < 1:     return f"{x:.4f}"
        return f"{x:.2f}"
    except Exception:
        return None
    
def one_liner(text: Optional[str], max_len: int = 120) -> Optional[str]:
    """ä»å¤šæ®µæ–‡æœ¬ä¸­æŠ½å–ä¸€å¥â€œæ€»ä½“ç»“è®ºâ€ï¼ˆé¦–å¥ï¼‰ï¼Œå¹¶è£å‰ªåˆ°åˆé€‚é•¿åº¦ã€‚"""
    if not text:
        return None
    try:
        s = re.sub(r"\s+", " ", str(text).strip())
        # ä»¥ä¸­æ–‡å¥å·/è‹±æ–‡å¥å·/é—®å·/å¹å·åˆ‡é¦–å¥
        import re as _re
        first = _re.split(r"[ã€‚.!?]\s*", s, maxsplit=1)[0] or s
        return (first[:max_len] + ("â€¦" if len(first) > max_len else ""))
    except Exception:
        return str(text)[:max_len]

def match_company(text: str) -> Optional[Dict[str, Any]]:
    if not text: return None
    _reload_caches()
    t = _norm(text)
    best, score = None, -1
    for row in _COMPANIES:
        names = [row.get("display_name") or "", row.get("company_id") or ""] + _split_aliases(row.get("aliases"))
        for n in names:
            nn = _norm(n); s = 0
            if t == nn: s = 100
            elif t in nn or nn in t: s = 60
            else:
                a = set([t[i:i+2] for i in range(len(t)-1)]) if len(t) > 1 else {t}
                b = set([nn[i:i+2] for i in range(len(nn)-1)]) if len(nn) > 1 else {nn}
                inter = len(a & b); uni = len(a | b) or 1
                s = 50 * inter / uni
            if s > score: score, best = s, row
    return best

def canonical_metric(text: str) -> Optional[str]:
    if not text: return None
    _reload_caches()
    t = _norm(text)
    best, score = None, -1
    for row in _METRIC_ALIASES:
        canonical = row.get("canonical_name") or ""
        names = [canonical] + _split_aliases(row.get("aliases"))
        for n in names:
            nn = _norm(n); s = 0
            if t == nn: s = 100
            elif t in nn or nn in t: s = 60
            if s > score: score, best = s, canonical
    return best

def canon_from_key_or_alias(name: str) -> Optional[str]:
    _reload_caches()
    if name in _KEY2CANON: return _KEY2CANON[name]
    if name in _ALIAS2CANON: return _ALIAS2CANON[name]
    return None

def get_children(parent: Dict[str, Any] | str) -> List[Dict[str, Any]]:
    """
    ä¸¥æ ¼çˆ¶å­å…³ç³»ï¼šä»…ç”¨ company_catalog çš„ id ä¸ parent_id ç²¾ç¡®åŒ¹é…ã€‚
    - parent å¯ä¼ å…¬å¸è¡Œæˆ–å…¶ idï¼ˆå­—ç¬¦ä¸²ï¼‰
    - è¿”å›æ‰€æœ‰æ»¡è¶³ row.parent_id == parent.id çš„å…¬å¸è¡Œ
    - ç»“æœæŒ‰ display_name å»é‡ï¼Œé¿å… catalog å­˜åœ¨åŒåä¸åŒ id å¯¼è‡´é‡å¤/ä¸¢å¤±
    """
    _reload_caches()

    if isinstance(parent, dict):
        pid = str(parent.get("id") or "").strip()
    else:
        pid = str(parent or "").strip()

    if not pid:
        return []

    # 1) ç²¾ç¡® parent_id åŒ¹é…
    children = [r for r in _COMPANIES if str(r.get("parent_id") or "").strip() == pid]

    # 2) å»é‡
    def _norm_name(x: str) -> str:
        return re.sub(r"\s+", "", str(x or "")).lower()

    seen_names, seen_ids, out = set(), set(), []
    for c in children:
        cid = str(c.get("id") or "").strip()
        cname = _norm_name(c.get("display_name") or "")
        if not cname:
            continue
        if cname in seen_names:
            continue
        if cid and cid in seen_ids:
            continue
        out.append(c)
        seen_names.add(cname)
        if cid:
            seen_ids.add(cid)
    return out






def find_formula(metric_name: str, label: Optional[str] = None, is_standard: Optional[bool] = None) -> Optional[Dict[str, Any]]:
    _reload_caches()
    for f in _FORMULAS:
        if (f.get("metric_name") == metric_name and
            (label is None or f.get("formula_label") == label) and
            (is_standard is None or bool(f.get("is_standard")) == bool(is_standard)) and
            bool(f.get("enabled", True))):
            return f
    return None

def fetch_metric_row_by_name(company_name: str, metric_name: str, year: int, quarter_int: int) -> Optional[Dict[str, Any]]:
    rows = _sb_select("financial_metrics", {
        "company_name": f"eq.{company_name}",
        "metric_name": f"eq.{metric_name}",
        "year": f"eq.{year}",
        "quarter": f"eq.{quarter_int}",
        "limit": 1
    })
    return rows[0] if rows else None

def list_company_metrics_by_name(company_name: str, year: int, quarter_int: int) -> List[Dict[str, Any]]:
    return _sb_select("financial_metrics", {
        "company_name": f"eq.{company_name}",
        "year": f"eq.{year}",
        "quarter": f"eq.{quarter_int}",
        "order": "metric_name.asc"
    })


def llm_summarize(sections: List[Dict[str, Any]], tone: str = "analytical") -> Optional[str]:
    if not (LLM_BASE and LLM_KEY and LLM_MODEL):
        return None

    system = ("ä½ æ˜¯èµ„æ·±ä¼ä¸šè´¢åŠ¡åˆ†æå¸ˆã€‚åŸºäºç»™å®šçš„ä¸‹é’»ç»“æœï¼Œé¢å‘é«˜ç®¡å†™ä¸€ä¸ªç®€æ´è€Œæœ‰æ¡ç†çš„ä¸­æ–‡æ€»ç»“ï¼Œ"
              "åŒ…å«ï¼šæ€»ä½“ç»“è®ºã€ä¸€å¥è¯å½’å› ã€å»ºè®®çš„ä¸‹ä¸€æ­¥ï¼ˆæœ€å¤š3æ¡ï¼‰ã€‚é¿å…é‡å¤åŸæ–‡æ•°å­—ï¼Œå°½é‡åšæç‚¼ã€‚")
    user = json.dumps(sections, ensure_ascii=False, indent=2)

    try:
        # 1) æ¨¡å‹å†³å®šç«¯ç‚¹
        endpoint = "/responses" if str(LLM_MODEL).lower().startswith(("gpt-5", "o4", "o3")) else "/chat/completions"
        url = f"{LLM_BASE.rstrip('/')}{endpoint}"

        # 2) payload
        if endpoint == "/responses":
            payload = {
                "model": LLM_MODEL,
                "input": [
                    {"role": "system", "content": system},
                    {"role": "user",   "content": user}
                ]
            }
        else:
            payload = {
                "model": LLM_MODEL,
                "messages": [
                    {"role": "system", "content": system},
                    {"role": "user",   "content": user}
                ],
                "temperature": 0.2
            }

        r = requests.post(url,
            headers={"Authorization": f"Bearer {LLM_KEY}", "Content-Type": "application/json"},
            json=payload, timeout=(LLM_CONNECT_TIMEOUT, LLM_READ_TIMEOUT))
        r.raise_for_status()

        data = r.json()
        content = data.get("output_text") if endpoint == "/responses" else data["choices"][0]["message"]["content"]
        return (content or "").strip()

    except Exception as e:
        print("[llm summarize warn]", e)


def get_indicator_card(question: str, company: Optional[str], metric: Optional[str], year: Optional[int], quarter: Optional[str]) -> Dict[str, Any]:
    # è‹¥å››è¦ç´ é½ï¼ŒæŠŠ question ç½®ç©ºï¼Œå¼ºåˆ¶ dataquery èµ°â€œç¡®å®šæ€§è·¯å¾„â€
    q = (question or "").strip()
    if company and metric and year and quarter:
        q = ""
    payload = {"question": q, "company": company, "metric": metric, "year": year, "quarter": quarter, "scenario": "actual"}
    r = requests.post(
        f"{DATA_AGENT_BASE_URL}/metrics/query",
        headers=_down_headers(DATA_AGENT_TOKEN),
        json=payload,
        timeout=30
    )
    if r.status_code >= 400: 
        raise HTTPException(502, f"dataquery_agent è°ƒç”¨å¤±è´¥: {r.text}")
    return r.json()


def safe_eval_compute(expr: str, vals: Dict[str, float]) -> float:
    allow = set("0123456789.+-*/() ")
    cleaned = []; i = 0
    while i < len(expr):
        ch = expr[i]
        if ch.isalpha() or ch == "_":
            j = i+1
            while j < len(expr) and (expr[j].isalnum() or expr[j] == "_"): j += 1
            name = expr[i:j]
            if name not in vals: raise ValueError(f"æœªçŸ¥å˜é‡: {name}")
            cleaned.append(str(vals[name])); i = j; continue
        if ch in allow: cleaned.append(ch); i += 1; continue
        raise ValueError(f"éæ³•å­—ç¬¦: {ch}")
    return float(eval("".join(cleaned), {"__builtins__": {}}))

def contribution_by_variables(expr: str, base_vals: Dict[str, float], new_vals: Dict[str, float]) -> List[Dict[str, Any]]:
    res = []
    try:
        base_y = safe_eval_compute(expr, base_vals)
        new_y  = safe_eval_compute(expr, new_vals)
    except Exception as e:
        return [{"variable":"_error","message":f"è®¡ç®—å¤±è´¥: {e}"}]
    total_delta = new_y - base_y
    for k in new_vals.keys():
        mid = dict(base_vals); mid[k] = new_vals[k]
        try:
            mid_y = safe_eval_compute(expr, mid); impact = mid_y - base_y
        except Exception:
            impact = None
        res.append({"variable":k,"base":base_vals.get(k),"new":new_vals.get(k),"impact_estimate":impact})
    res.append({"variable":"_total","impact_estimate":total_delta})
    return res

def _parse_formula(f: Dict[str, Any]) -> Dict[str, Any]:
    vars_raw = f.get("variables"); comp_raw = f.get("compute")
    if isinstance(vars_raw, str):
        try: vars_raw = json.loads(vars_raw)
        except Exception: pass
    if isinstance(vars_raw, dict):
        var_keys = list(vars_raw.keys()); var_cn_map = dict(vars_raw)
    elif isinstance(vars_raw, list):
        var_keys = [str(x) for x in vars_raw]; var_cn_map = {k:k for k in var_keys}
    else:
        var_keys, var_cn_map = [], {}
    if isinstance(comp_raw, str): expr = comp_raw
    elif isinstance(comp_raw, dict):
        keys = list(comp_raw.keys())
        prefer = None
        for k in ["roe","result","value","y","metric"]:
            if k in comp_raw:
                prefer = k; break
        expr = comp_raw.get(prefer, comp_raw.get(keys[-1], ""))

    else: expr = ""
    used_tokens = set(re.findall(r"\b[a-zA-Z_]\w*\b", expr))
    ordered_vars = [k for k in var_keys if k in used_tokens] or var_keys
    return {"expr":expr,"var_keys":ordered_vars,"var_cn_map":var_cn_map}

# â€”â€” æ–°å¢ï¼šcompute_key â†’ ä¸­æ–‡å…¬å¼
def expr_to_cn(expr: str, cn_map: Dict[str, str]) -> str:
    if not expr: return expr
    tokens = sorted(set(re.findall(r"\b[a-zA-Z_]\w*\b", expr)), key=len, reverse=True)
    out = expr
    for k in tokens:
        cn = canon_from_key_or_alias(k) or cn_map.get(k, k)
        out = re.sub(rf"\b{k}\b", cn, out)
    return out

# â€”â€” æ–°å¢ï¼šå…œåº•æ–‡å­—æ€»ç»“
def build_basic_summary(indicator_card: Optional[Dict[str, Any]], sections: List[Dict[str, Any]]) -> str:
    lines: List[str] = []
    if indicator_card:
        name = indicator_card.get("metric") or "è¯¥æŒ‡æ ‡"
        comp = indicator_card.get("company") or ""
        when = indicator_card.get("time") or ""
        yoy = indicator_card.get("yoy_delta_str") or indicator_card.get("yoy_delta")
        qoq = indicator_card.get("qoq_delta_str") or indicator_card.get("qoq_delta")
        tgt = indicator_card.get("target_gap_str")
        p1 = f"{comp}Â·{when} çš„ã€Œ{name}ã€å·²å®Œæˆæœ€æ–°å–æ•°ã€‚"
        if yoy is not None or qoq is not None:
            p1 += f" åŒæ¯”ï¼š{yoy if yoy is not None else '-'}ï¼›ç¯æ¯”ï¼š{qoq if qoq is not None else '-'}ã€‚"
        if tgt is not None: p1 += f" ä¸ç›®æ ‡å·®è·ï¼š{tgt}ã€‚"
        lines.append(p1)
    for s in sections:
        t = s.get("type")
        if t == "dimension" and s.get("conclusion"):
            ytop = s["conclusion"].get("yoy_top") or []
            qtop = s["conclusion"].get("qoq_top") or []
            if ytop: lines.append(f"åŒæ¯”çœ‹ï¼Œã€Œ{ytop[0]['company']}ã€è´¡çŒ®/æ‹–ç´¯æœ€å¤§ï¼ˆÎ”={ytop[0].get('yoy_delta_str') or fmt_num(ytop[0].get('yoy_delta'))}ï¼‰ã€‚")
            if qtop: lines.append(f"ç¯æ¯”çœ‹ï¼Œã€Œ{qtop[0]['company']}ã€è´¡çŒ®/æ‹–ç´¯æœ€å¤§ï¼ˆÎ”={qtop[0].get('qoq_delta_str') or fmt_num(qtop[0].get('qoq_delta'))}ï¼‰ã€‚")
        if t in {"metric","business"}:
            rows = s.get("contribution_yoy") or []
            rows = [r for r in rows if r.get("variable") not in {"åˆè®¡"} and isinstance(r.get("impact_raw"), (int,float))]
            rows.sort(key=lambda r: abs(r.get("impact_raw",0)), reverse=True)
            if rows[:2]:
                k = "ï¼›".join([f"ã€Œ{r['variable']}ã€â‰ˆ{r.get('impact') or fmt_num(r.get('impact_raw'))}" for r in rows[:2]])
                lines.append(f"åˆ†é¡¹å½’å› ï¼šä¸»è¦ç”± {k} é©±åŠ¨ï¼ˆä¼°ç®—è´¡çŒ®ï¼‰ã€‚")
        if t == "anomaly":
            ty = (s.get("top_yoy") or [])[:1]; tq = (s.get("top_qoq") or [])[:1]
            if ty: lines.append(f"åŒæ¯”å¼‚åŠ¨é¦–ä½ï¼š{ty[0]['metric']}ï¼ˆÎ”={ty[0].get('yoy_change_str') or fmt_num(ty[0].get('yoy_change'))}ï¼‰ã€‚")
            if tq: lines.append(f"ç¯æ¯”å¼‚åŠ¨é¦–ä½ï¼š{tq[0]['metric']}ï¼ˆÎ”={tq[0].get('qoq_change_str') or fmt_num(tq[0].get('qoq_change'))}ï¼‰ã€‚")
    if sections: lines.append("å»ºè®®ï¼šâ‘  å¯¹è´¡çŒ®æœ€å¤§çš„åˆ†é¡¹/å­å…¬å¸åšæ˜ç»†å¤æ ¸ï¼›â‘¡ æ ¡éªŒå£å¾„ä¸ä¸€æ¬¡æ€§å› ç´ ï¼›â‘¢ å¦‚æœ‰ç›®æ ‡å·®è·ï¼Œåˆ¶å®šä¸“é¡¹è¿½èµ¶æ–¹æ¡ˆã€‚")
    return "\n".join(lines)

def _wrap_contrib_rows(contrib: List[Dict[str, Any]], cn_map: Dict[str,str]) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    for item in contrib:
        key = item.get("variable")
        if key == "_error": rows.append(item); continue
        cname = "åˆè®¡" if key == "_total" else (canon_from_key_or_alias(key) or cn_map.get(key) or key)
        base = item.get("base"); newv = item.get("new"); imp = item.get("impact_estimate")
        rows.append({
            "variable": cname, "base": fmt_num(base), "new": fmt_num(newv), "impact": fmt_num(imp),
            "variable_key": key, "base_raw": base, "new_raw": newv, "impact_raw": imp,
        })
    return rows

def _drill_dimension(company_row: Dict[str, Any], metric_name: str, year: int, quarter_int: int, top_k: int = 3) -> Dict[str, Any]:

    """
    ç»´åº¦ä¸‹é’»ï¼ˆä¸¥æ ¼è¦æ±‚ï¼‰ï¼š
    1) ç”¨ company_catalog.id â†’ parent_id æ‰¾åˆ°æ‰€æœ‰å­å…¬å¸ï¼›
    2) æ‹¿å­å…¬å¸ display_nameï¼›
    3) é€ä¸ªè°ƒç”¨ dataquery_agent /metrics/query å– {current, yoy_delta, qoq_delta}ï¼›
    4) æ±‡æ€»è¡¨æ ¼ä¸**å•ä¸ª**é¥¼å›¾ï¼ˆé¿å…é‡å¤å›¾è¡¨ï¼‰ã€‚
    """
    children = get_children(company_row)
    found_names = [str(c.get("display_name") or "").strip() for c in children if c.get("display_name")]  # [ADD]
    probe: List[Dict[str, Any]] = []  # [ADD] é€ä¸ªå­å…¬å¸å–æ•°çš„æˆåŠŸ/å¤±è´¥è®°å½•


    if not children:
        return {
            "type": "dimension",
            "title": "ç»´åº¦ä¸‹é’»",
            "message": "æ²¡æœ‰å­å…¬å¸å¯ä¸‹é’»ã€‚",
            "table": [],
            "chart": {"type": "pie", "data": []},
            "conclusion": {"yoy_top": [], "qoq_top": []},
            "debug": {"children_found": [], "data_calls": []}  # [ADD]
        }


    rows = []
    for c in children:
        child_name = str(c.get("display_name") or "").strip()
        if not child_name:
            continue

        payload = {
            "question": "",                       # é¿å…å†èµ° LLM
            "company": child_name,
            "metric": metric_name,                # å·²åœ¨ä¸Šæ¸¸ canonical è¿‡
            "year": int(year),
            "quarter": f"Q{int(quarter_int)}",
            "scenario": "actual"
        }
        try:
            r = requests.post(
                f"{DATA_AGENT_BASE_URL}/metrics/query",
                headers=_down_headers(DATA_AGENT_TOKEN),
                json=payload,
                timeout=30
            )

            if r.status_code >= 400:
                probe.append({"name": child_name, "ok": False, "reason": f"HTTP {r.status_code}"})  # [ADD]
                continue

            dq = r.json() or {}
            card = dq.get("indicator_card") or {}
            cur = card.get("current")
            yoy = card.get("yoy_delta")
            qoq = card.get("qoq_delta")

            if cur is None and yoy is None and qoq is None:
                probe.append({"name": child_name, "ok": False, "reason": "no values"})  # [ADD]
                continue

            rows.append({
                "company": child_name,
                "current": cur, "current_str": fmt_num(cur),
                "yoy_delta": yoy, "yoy_delta_str": fmt_num(yoy),
                "qoq_delta": qoq, "qoq_delta_str": fmt_num(qoq),
            })
            probe.append({"name": child_name, "ok": True, "current": cur})  # [ADD]
        except Exception as e:
            probe.append({"name": child_name, "ok": False, "reason": str(e)})  # [ADD]
            continue

    if not rows:
        return {
            "type": "dimension",
            "title": "ç»´åº¦ä¸‹é’»",
            "message": "å­å…¬å¸åœ¨è¯¥æœŸæœªæ£€ç´¢åˆ°æœ‰æ•ˆæ•°æ®ã€‚",
            "table": [],
            "chart": {"type": "pie", "data": []},
            "conclusion": {"yoy_top": [], "qoq_top": []},
            "debug": {"children_found": found_names, "data_calls": probe}  # [ADD]
        }


    # TOPï¼ˆæŒ‰ç»å¯¹å˜åŠ¨ï¼‰
    def _abs_or_neg1(v): return abs(v) if isinstance(v, (int, float)) else -1
    yoy_top = sorted(rows, key=lambda x: _abs_or_neg1(x.get("yoy_delta")), reverse=True)[:max(1, int(top_k))]
    qoq_top = sorted(rows, key=lambda x: _abs_or_neg1(x.get("qoq_delta")), reverse=True)[:max(1, int(top_k))]

    # **åªç»˜åˆ¶ä¸€ä¸ª**é¥¼å›¾ï¼ˆå½“å‰å€¼å æ¯”ï¼‰
    chart = {"type": "pie", "data": [{"name": r["company"], "value": r.get("current") or 0} for r in rows]}

    ok_names = [p["name"] for p in probe if p.get("ok")]
    fail_names = [p["name"] for p in probe if not p.get("ok")]
    tip = f"ï¼ˆå­å…¬å¸å…±{len(found_names)}å®¶ï¼ŒæˆåŠŸ{len(ok_names)}ï¼Œæœªå‘½ä¸­{len(fail_names)}ï¼‰"

    # ğŸ‘‡ ç²¾ç®€æ¨¡å¼ï¼šéšè—å®Œæ•´è¡¨ï¼Œä»…ä¿ç•™ TOP å’Œé¥¼å›¾ï¼›å®Œæ•´è¡¨å¡åˆ° debug.table_full
    table_to_return = [] if COMPACT_DIMENSION_TABLES else rows
    debug_extra = {
        "children_found": found_names,
        "data_calls": probe,
        "table_full": rows if COMPACT_DIMENSION_TABLES else None
    }

    return {
        "type": "dimension",
        "title": "ç»´åº¦ä¸‹é’»",
        "message": f"ä»å­å…¬å¸å±‚é¢çœ‹ï¼Œå±•ç¤ºåŒæ¯”/ç¯æ¯” TOP{max(1, int(top_k))} ä¸å½“å‰å€¼å æ¯”é¥¼å›¾ã€‚" + tip,
        "conclusion": {"yoy_top": yoy_top, "qoq_top": qoq_top},
        "table": table_to_return,       # â† ç²¾ç®€ï¼šè¿™é‡Œä¸ºç©ºæ•°ç»„æ—¶å‰ç«¯è‡ªç„¶ä¸å†æ¸²æŸ“ç¬¬ä¸‰å¼ è¡¨
        "chart": chart,
        "debug": debug_extra
    }






def _drill_metric(company_row: Dict[str, Any], metric_name: str, year: int, quarter_int: int) -> Dict[str, Any]:
    f = find_formula(metric_name, is_standard=True) or find_formula(metric_name, label="æ ‡å‡†å…¬å¼")
    if not f: return {"type":"metric","title":"æŒ‡æ ‡ä¸‹é’»","message": f"æœªæ‰¾åˆ°ã€{metric_name}ã€çš„æ ‡å‡†å…¬å¼ï¼Œæ— æ³•æŒ‡æ ‡ä¸‹é’»ã€‚"}
    pf = _parse_formula(f); expr = pf["expr"]; var_keys = pf["var_keys"]; cn_map = pf["var_cn_map"]
    if not expr or not var_keys: return {"type":"metric","title":"æŒ‡æ ‡ä¸‹é’»","message":"æ ‡å‡†å…¬å¼å®šä¹‰ä¸å®Œæ•´ï¼ˆç¼ºå°‘ variables/computeï¼‰ã€‚"}

    # ğŸ”§ å…œåº•ï¼šè‹¥ variables é‡Œç¼ºå°‘æŸè®¡ç®—é”®ï¼ŒæŒ‰åˆ«åè¡¨è¡¥æˆä¸­æ–‡ï¼ˆå¦‚ net_profit_margin â†’ å‡€åˆ©ç‡ï¼‰
    for k in set(re.findall(r"\b[a-zA-Z_]\w*\b", expr)):
        if k not in cn_map:
            cn = canon_from_key_or_alias(k)
            if cn: cn_map[k] = cn

    base_vals, new_vals = {}, {}
    for k in var_keys:
        cn = cn_map.get(k, k)
        r_cur = fetch_metric_row_by_name(company_row.get("display_name"), cn, year, quarter_int)
        if r_cur and r_cur.get("metric_value") is not None and r_cur.get("last_year_value") is not None:
            new_vals[k]  = r_cur.get("metric_value"); base_vals[k] = r_cur.get("last_year_value")

    tokens = set(re.findall(r"\b[a-zA-Z_]\w*\b", expr))
    new_vals = {k:v for k,v in new_vals.items() if k in tokens}
    base_vals = {k:v for k,v in base_vals.items() if k in tokens}

    contrib_rows = _wrap_contrib_rows(contribution_by_variables(expr, base_vals, new_vals), cn_map)
    return {
        "type":"metric","title":"æŒ‡æ ‡ä¸‹é’»ï¼ˆæ ‡å‡†å…¬å¼ï¼‰",
        "formula":{
            "variables":[canon_from_key_or_alias(k) or cn_map.get(k, k) for k in var_keys],
            "variables_cn":[canon_from_key_or_alias(k) or cn_map.get(k, k) for k in var_keys],
            "compute":expr,"compute_cn":expr_to_cn(expr, cn_map)
        },
        "contribution_yoy":contrib_rows,
        "note":"è´¡çŒ®ä¼°ç®—åŸºäºé€ä¸ªå˜é‡æ›¿æ¢æ³•ï¼Œä½œä¸ºå®šæ€§è§£é‡Šã€‚"
    }

def _drill_business(company_row: Dict[str, Any], metric_name_for_biz: str, year: int, quarter_int: int) -> Dict[str, Any]:
    f = find_formula(metric_name_for_biz, label="ä¸šåŠ¡å…¬å¼") or find_formula(metric_name_for_biz, is_standard=False)
    if not f: return {"type":"business","title":"ä¸šåŠ¡ä¸‹é’»","message": f"æœªæ‰¾åˆ°ã€{metric_name_for_biz}ã€çš„ä¸šåŠ¡å…¬å¼ã€‚"}
    pf = _parse_formula(f); expr = pf["expr"]; var_keys = pf["var_keys"]; cn_map = pf["var_cn_map"]
    if not expr or not var_keys: return {"type":"business","title":"ä¸šåŠ¡ä¸‹é’»","message":"ä¸šåŠ¡å…¬å¼å®šä¹‰ä¸å®Œæ•´ï¼ˆç¼ºå°‘ variables/computeï¼‰ã€‚"}

    # ğŸ”§ å…œåº•ï¼šæŠŠç¼ºå¤±çš„è®¡ç®—é”®è¡¥æˆä¸­æ–‡
    for k in set(re.findall(r"\b[a-zA-Z_]\w*\b", expr)):
        if k not in cn_map:
            cn = canon_from_key_or_alias(k)
            if cn: cn_map[k] = cn

    base_vals, new_vals = {}, {}
    for k in var_keys:
        cn = cn_map.get(k, k)
        r_cur = fetch_metric_row_by_name(company_row.get("display_name"), cn, year, quarter_int)
        if r_cur and r_cur.get("metric_value") is not None and r_cur.get("last_year_value") is not None:
            new_vals[k]  = r_cur.get("metric_value"); base_vals[k] = r_cur.get("last_year_value")

    tokens = set(re.findall(r"\b[a-zA-Z_]\w*\b", expr))
    new_vals = {k:v for k,v in new_vals.items() if k in tokens}
    base_vals = {k:v for k,v in base_vals.items() if k in tokens}

    contrib_rows = _wrap_contrib_rows(contribution_by_variables(expr, base_vals, new_vals), cn_map)
    return {
        "type":"business","title":f"ä¸šåŠ¡ä¸‹é’»ï¼ˆ{metric_name_for_biz}ï¼‰",
        "formula":{
            "variables":[canon_from_key_or_alias(k) or cn_map.get(k, k) for k in var_keys],
            "variables_cn":[canon_from_key_or_alias(k) or cn_map.get(k, k) for k in var_keys],
            "compute":expr,"compute_cn":expr_to_cn(expr, cn_map)
        },
        "contribution_yoy":contrib_rows,
        "note":"è´¡çŒ®ä¼°ç®—åŸºäºé€ä¸ªå˜é‡æ›¿æ¢æ³•ï¼Œä½œä¸ºå®šæ€§è§£é‡Šã€‚"
    }

class DrillMode(str, Enum):
    dimension="dimension"; metric="metric"; business="business"; anomaly="anomaly"

class AnalyzeReq(BaseModel):
    question: str
    company: Optional[str] = None
    metric: Optional[str] = None
    year: Optional[int] = None
    quarter: Optional[str] = None
    modes: List[DrillMode] = Field(default_factory=list)
    business_formula_metric_name: Optional[str] = None
    top_k: int = 3
    # æ–°å¢ï¼šæ§åˆ¶æ”¿ç­–æ®µ
    skip_policy: bool = False
    policy_only: bool = False


class AnalyzeResp(BaseModel):
    indicator_card: Optional[Dict[str, Any]] = None
    resolved: Optional[Dict[str, Any]] = None
    sections: List[Dict[str, Any]] = Field(default_factory=list)
    summary: Optional[str] = None
    # æ–°å¢ï¼šè¿è¡Œè¿›åº¦ï¼ˆå‰ç«¯å¯ç›´æ¥æ¸²æŸ“ï¼‰
    progress: List[Dict[str, Any]] = Field(default_factory=list)
# === æ–°å¢ï¼šæ ¸å¿ƒæ‰§è¡Œå‡½æ•°ï¼Œå¯è¢«åŒæ­¥/æµå¼ä¸¤ä¸ªå…¥å£å¤ç”¨ ===
def _analyze_core(req: AnalyzeReq, on_push=None) -> AnalyzeResp:
    """
    on_push: å¯é€‰å›è°ƒï¼Œå½¢å¦‚ on_push(event_dict)ï¼Œç”¨äºå°†æ¯ä¸€æ­¥è¿›åº¦å‘å¤–æ¨é€ï¼›
             event_dict = {"step": "...", "status": "start|done|error", "elapsed_ms": int, "detail": "..."}
    è¿”å›å€¼ï¼šAnalyzeRespï¼ˆä¸ä½ åŸ analyze çš„è¿”å›ä¸€è‡´ï¼‰
    """
    t0 = time.time()
    progress: List[Dict[str, Any]] = []
    def push(step: str, status: str, detail: Optional[str] = None):
        ev = {
            "step": step,
            "status": status,
            "elapsed_ms": int((time.time() - t0) * 1000),
            **({"detail": str(detail)} if detail else {})
        }
        progress.append(ev)
        if callable(on_push):
            try: on_push(ev)
            except Exception: pass

    # â€”â€” ä¸‹é¢è¿™æ®µé€»è¾‘ï¼ŒåŸºæœ¬ç…§æ¬ä½ åŸ analyze() çš„ä¸»ä½“ï¼ŒåªæŠŠâ€œpush(...)â€æ›¿æ¢ä¸ºä¸Šé¢å®šä¹‰çš„ push â€”â€” #
    # 1) å–æ•°ï¼ˆæŒ‡æ ‡å¡ï¼‰
    push("å–æ•°ä¸­", "start")
    dq = get_indicator_card(req.question, req.company, req.metric, req.year, req.quarter)
    push("å–æ•°ä¸­", "done")
    indicator_card = dq.get("indicator_card")

    resolved_dq = dq.get("resolved") or {}
    company_name = resolved_dq.get("company") or resolved_dq.get("company_name") or req.company
    metric_name  = resolved_dq.get("metric")  or resolved_dq.get("metric_canonical") or req.metric
    year         = int(resolved_dq.get("year") or req.year or 2025)
    q_str        = str(resolved_dq.get("quarter") or req.quarter or "Q2").upper()
    quarter_int  = int(q_str.replace("Q", "")) if "Q" in q_str else int(q_str)

    comp_row = match_company(company_name or "")
    if not comp_row:
        raise HTTPException(404, f"æ— æ³•è¯†åˆ«å…¬å¸ï¼š{company_name}")
    canon_metric = canonical_metric(metric_name or "") or (metric_name or "")

    sections: List[Dict[str, Any]] = []

    # (A) è§„åˆ’
    push("åˆ†æé—®é¢˜ä¸­ï¼ˆæ„å›¾è¯†åˆ«/è§„åˆ’ï¼‰", "start")
    try:
        plan_ctx = {
            "company": comp_row.get("display_name") or comp_row.get("company_id"),
            "metric": canon_metric, "year": year, "quarter": quarter_int,
            "modes": [m.value for m in req.modes],
        }
        plan = llm_chat(PROMPT_PLANNER, json.dumps(plan_ctx, ensure_ascii=False), temperature=0.2) or ""
        push("åˆ†æé—®é¢˜ä¸­ï¼ˆæ„å›¾è¯†åˆ«/è§„åˆ’ï¼‰", "done",
             detail=(str(plan).strip()[:300] + ("..." if len(str(plan)) > 300 else "")) if plan else None)
    except Exception as e:
        push("åˆ†æé—®é¢˜ä¸­ï¼ˆæ„å›¾è¯†åˆ«/è§„åˆ’ï¼‰", "error", detail=e)

    # (B) ä¸‹é’»
    if not req.policy_only and req.modes:
        push("ä¸‹é’»æ‰§è¡Œä¸­", "start")
        for mode in req.modes:
            if mode == DrillMode.dimension:
                sections.append(_drill_dimension(comp_row, canon_metric, year, quarter_int, max(1, int(req.top_k))))
            elif mode == DrillMode.metric:
                sections.append(_drill_metric(comp_row, canon_metric, year, quarter_int))
            elif mode == DrillMode.business:
                biz_metric = req.business_formula_metric_name or canon_metric
                sections.append(_drill_business(comp_row, biz_metric, year, quarter_int))
            elif mode == DrillMode.anomaly:
                sections.append(_drill_anomaly(comp_row, year, quarter_int, max(1, int(req.top_k))))
            else:
                sections.append({"type": "unknown", "message": f"æœªçŸ¥æ¨¡å¼ï¼š{mode}"})
        push("ä¸‹é’»æ‰§è¡Œä¸­", "done")

    #     # (C) æ”¿ç­–ä¸Šä¸‹æ–‡
    # if not req.skip_policy:
    #     try:
    #         # â‘  å…ˆåšâ€œæ”¿ç­–å€™é€‰æ£€ç´¢â€å¹¶æŠŠç»“æœæ˜ç¡®å†™è¿›è¿›åº¦ä¸ sections
    #         push("æ”¿ç­–æ£€ç´¢ï¼ˆå€™é€‰ï¼‰", "start")
    #         industry = (comp_row.get("business_unit")) or None

    #         policy_hits = []
    #         # if not (GOOGLE_API_KEY and GOOGLE_CSE_ID):
    #         #     # æœªé…ç½®å¯†é’¥ï¼šæ˜ç¡®å‘Šè¯‰å‰ç«¯â€œä¸ºä»€ä¹ˆæ²¡æœ‰å»æ£€ç´¢â€
    #         #     push("æ”¿ç­–æ£€ç´¢ï¼ˆå€™é€‰ï¼‰", "done", detail="æœªé…ç½® GOOGLE_API_KEY/CSE_IDï¼Œè·³è¿‡æ£€ç´¢")
    #         #     sections.append({
    #         #         "type": "policy_info",
    #         #         "title": "æ”¿ç­–æ£€ç´¢çŠ¶æ€",
    #         #         "message": "æœªé…ç½® GOOGLE_API_KEY/CSE_IDï¼Œè·³è¿‡æ”¿ç­–å€™é€‰æ£€ç´¢ï¼›ä¸‹æ–‡ä»…ç»™å‡ºé€šç”¨æ¡†æ¶ã€‚"
    #         #     })
    #         # else:
    #         #     try:
    #         #         policy_hits = _google_cse_policy_search(
    #         #             company_name=plan_ctx["company"],
    #         #             industry=industry,
    #         #             year=year,
    #         #             quarter=f"Q{quarter_int}",
    #         #             limit=6
    #         #         )
    #         #         push("æ”¿ç­–æ£€ç´¢ï¼ˆå€™é€‰ï¼‰", "done", detail=f"{len(policy_hits)} æ¡")
    #         #     except Exception as e:
    #         #         # æ£€ç´¢å¼‚å¸¸ï¼šä¹Ÿè¦æŠŠåŸå› å›ä¼ åˆ°è¿›åº¦é‡Œ
    #         #         policy_hits = []
    #         #         push("æ”¿ç­–æ£€ç´¢ï¼ˆå€™é€‰ï¼‰", "done", detail=f"æ£€ç´¢å¤±è´¥ï¼š{e}")
    #         #         sections.append({
    #         #             "type": "policy_info",
    #         #             "title": "æ”¿ç­–æ£€ç´¢çŠ¶æ€",
    #         #             "message": f"æ”¿ç­–å€™é€‰æ£€ç´¢å¤±è´¥ï¼š{e}"
    #         #         })
    #         # # æŠŠå€™é€‰æ¸…å•å•ç‹¬è½ä¸€èŠ‚ï¼ˆå…ˆå±•ç¤ºåˆ—è¡¨ï¼Œå†åšå½±å“åˆ†æï¼‰
    #         # # â€”â€” åŸæ¥è¿™é‡Œç›´æ¥å¼€å§‹æ‹¼ policy_candidates / åšæ”¿ç­–ä¸Šä¸‹æ–‡ â€”â€” 
    #         # # ç°åœ¨æ”¹æˆï¼š
    #         # if not req.skip_policy:
    #         #     # æŠŠå€™é€‰æ¸…å•å•ç‹¬è½ä¸€èŠ‚ï¼ˆå…ˆå±•ç¤ºåˆ—è¡¨ï¼Œå†åšå½±å“åˆ†æï¼‰
    #         #     if policy_hits:
    #         #         sections.append({
    #         #             "type": "policy_candidates",
    #         #             "title": "æ”¿ç­–å€™é€‰æ¸…å•",
    #         #             "table": [{"title": h.get("title"), "source": h.get("source"), "snippet": h.get("snippet")} for h in policy_hits]
    #         #         })

    #         #     # â‘¡ å†åšâ€œæ”¿ç­–ä¸Šä¸‹æ–‡/å½±å“è·¯å¾„â€çš„ LLM å½’çº³
    #         #     push("è°ƒç”¨åˆ†æagentå¤§æ¨¡å‹ä¸­ï¼ˆæ”¿ç­–ä¸Šä¸‹æ–‡ï¼‰", "start")
    #         #     pol_ctx = {
    #         #         "resolved": {"company": plan_ctx["company"], "metric": canon_metric, "year": year, "quarter": quarter_int},
    #         #         "sections": sections,
    #         #         "policy_news": policy_hits
    #         #     }
    #         #     pol_json = llm_chat(PROMPT_POLICY, json.dumps(pol_ctx, ensure_ascii=False), want_json=True, temperature=0.3)
    #         #     if isinstance(pol_json, dict) and (pol_json.get("message") or pol_json.get("table") or pol_json.get("chart")):
    #         #         sections.append({"type": "policy", **pol_json})
    #         #     push("è°ƒç”¨åˆ†æagentå¤§æ¨¡å‹ä¸­ï¼ˆæ”¿ç­–ä¸Šä¸‹æ–‡ï¼‰", "done",
    #         #         detail=(pol_json.get("message")[:200] if isinstance(pol_json, dict) and pol_json.get("message") else None))
    #         # # â† è¿™ä¸€å¤§æ®µåŒ…è£¹ç»“æŸ

    #     except Exception as e:
    #         push("è°ƒç”¨åˆ†æagentå¤§æ¨¡å‹ä¸­ï¼ˆæ”¿ç­–ä¸Šä¸‹æ–‡ï¼‰", "error", detail=e)

    # if req.policy_only:
    #     return AnalyzeResp(indicator_card=None, resolved=plan_ctx, sections=sections, summary=None, progress=progress)

    # (D) æœ€ç»ˆæ•´ç†
    push("è°ƒç”¨åˆ†æagentå¤§æ¨¡å‹ä¸­ï¼ˆæœ€ç»ˆæ•´ç†ï¼‰", "start")
    final_ctx = {"indicator_card": indicator_card, "resolved": plan_ctx, "sections": sections}
    final_json = llm_chat(PROMPT_ANALYST, json.dumps(final_ctx, ensure_ascii=False), want_json=True, temperature=0.2)
    llm_summary = None
    if isinstance(final_json, dict):
        llm_summary = final_json.get("summary")
        extra = final_json.get("extra_sections") or []
        for s in extra:
            if isinstance(s, dict) and (s.get("message") or s.get("table") or s.get("chart")):
                sections.append(s)

    summary = llm_summary or llm_summarize(sections)
    if not summary:
        push("è°ƒç”¨åˆ†æagentå¤§æ¨¡å‹ä¸­ï¼ˆæœ€ç»ˆæ•´ç†ï¼‰", "error", detail="LLM ç”Ÿæˆå¤±è´¥")
        push("ç”Ÿæˆç»“æœä¸­", "error", detail="LLM ç”Ÿæˆå¤±è´¥")
        raise HTTPException(502, "LLM ç”Ÿæˆå¤±è´¥ï¼šæœªå¯ç”¨æˆ–æ¨¡å‹ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ .env çš„ OPENAI_* æˆ– LLM_*ã€‚")

    push("è°ƒç”¨åˆ†æagentå¤§æ¨¡å‹ä¸­ï¼ˆæœ€ç»ˆæ•´ç†ï¼‰", "done")

    push("ç”Ÿæˆç»“æœä¸­", "start")
    summary_one = one_liner(summary, max_len=120)
    push("ç”Ÿæˆç»“æœä¸­", "done", detail=summary_one or summary)

    return AnalyzeResp(
        indicator_card=indicator_card,
        resolved={
            "company": comp_row.get("display_name"),
            "metric": canon_metric,
            "year": year,
            "quarter": f"Q{quarter_int}",
            "modes": [m.value for m in req.modes]
        },
        sections=sections,
        summary=summary,
        progress=progress
    )

class BizFormula(BaseModel):
    metric_name: str
    description: Optional[str] = None
    variables: Optional[List[str]] = None
    compute: Optional[str] = None
    # ğŸ‘‡ æ–°å¢
    method: Optional[str] = None
    method_name: Optional[str] = None
    compute_cn: Optional[str] = None
    variables_cn: Optional[List[str]] = None


app = FastAPI(title="deepanalysis_agent", version="0.3.1")
# deepanalysis_agent.py
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://127.0.0.1:5173",
    ],
    allow_methods=["*"],
    allow_headers=["*"],
    allow_credentials=True,   # å…è®¸å«å‡­è¯è¯·æ±‚
)

ALLOWED_ORIGINS = {"http://localhost:5173", "http://127.0.0.1:5173"}

@app.options("/{path:path}")
async def options_handler(request: Request, path: str):
    origin = request.headers.get("origin")
    resp = Response(status_code=204)
    if origin in ALLOWED_ORIGINS:
        resp.headers.update({
            "Access-Control-Allow-Origin": origin,
            "Access-Control-Allow-Credentials": "true",
            "Access-Control-Allow-Methods": request.headers.get("Access-Control-Request-Method", "*"),
            "Access-Control-Allow-Headers": request.headers.get("Access-Control-Request-Headers", "*"),
        })
    return resp

@app.middleware("http")
async def add_cors_on_error(request: Request, call_next):
    origin = request.headers.get("origin")
    try:
        resp = await call_next(request)
    except Exception as e:
        # æŠŠ 500 åŒ…æˆ JSONï¼Œæ–¹ä¾¿å‰ç«¯çœ‹åˆ°çœŸå®é”™è¯¯
        resp = JSONResponse({"detail": str(e)}, status_code=500)
    if origin in ALLOWED_ORIGINS:
        resp.headers["Access-Control-Allow-Origin"] = origin
        resp.headers["Access-Control-Allow-Credentials"] = "true"
    return resp
@app.get("/llm/ping")
def llm_ping():
    if not (LLM_BASE and LLM_KEY and LLM_MODEL):
        raise HTTPException(400, "LLM æœªé…ç½®ï¼ˆOPENAI_* / LLM_*ï¼‰")
    try:
        r = requests.get(f"{LLM_BASE}/models",
                         headers={"Authorization": f"Bearer {LLM_KEY}"},
                         timeout=10)
        return {"ok": r.ok, "status": r.status_code, "model": LLM_MODEL, "base": LLM_BASE}
    except Exception as e:
        raise HTTPException(502, f"LLM ping failed: {e}")

def require_token(authorization: Optional[str] = Header(None)):
    if DEV_BYPASS_AUTH: return True
    if not authorization or not authorization.lower().startswith("bearer "):
        raise HTTPException(401, "Unauthorized")
    return True

@app.get("/business/formulas", response_model=List[BizFormula])
def list_business_formulas(_:bool=True):
    _reload_caches()
    out: List[BizFormula] = []
    method_label = {"dupont": "æœé‚¦åˆ†è§£", "ratio": "å…¬å¼æ³•"}

    for f in _FORMULAS:
        if f.get("formula_label") != "ä¸šåŠ¡å…¬å¼" or not f.get("enabled", True):
            continue

        pf = _parse_formula(f)  # è¿”å› {expr,var_keys,var_cn_map}ï¼Œä½† expr å¯èƒ½æ˜¯ç¬¬ä¸€æ¡
        comp_raw = f.get("compute")
        expr = ""
        if isinstance(comp_raw, dict):
            # âœ… ä¼˜å…ˆå–æœ€ç»ˆç›®æ ‡
            if "roe" in comp_raw: expr = comp_raw["roe"]
            elif "result" in comp_raw: expr = comp_raw["result"]
            elif "value" in comp_raw: expr = comp_raw["value"]
            else:
                # æ²¡æœ‰æ˜¾å¼é”®æ—¶å–â€œæœ€åä¸€æ¡â€
                try:
                    last_key = list(comp_raw.keys())[-1]
                    expr = comp_raw[last_key]
                except Exception:
                    expr = ""
        elif isinstance(comp_raw, str):
            expr = comp_raw

        # å˜é‡æ˜ å°„ï¼ˆä¸­æ–‡ï¼‰
        cn_map = dict(pf.get("var_cn_map") or {})
        # æœé‚¦å¸¸è§ç¼©å†™è¡¥å…¨
        if (f.get("method") or "").lower() == "dupont":
            cn_map.update({"npm": "å‡€åˆ©ç‡", "at": "æ€»èµ„äº§å‘¨è½¬ç‡", "em": "æƒç›Šä¹˜æ•°", "roe": "ROE"})

        compute_cn = expr_to_cn(expr, cn_map)
        method = (f.get("method") or "").lower()
        out.append(BizFormula(
            metric_name=f.get("metric_name"),
            description=f.get("description"),
            variables=list(cn_map.values()) or None,
            variables_cn=list(cn_map.values()) or None,
            compute=expr,
            compute_cn=compute_cn,
            method=method,
            method_name=method_label.get(method, f.get("method") or "ä¸šåŠ¡å…¬å¼"),
        ))
    return out


def _drill_anomaly(company_row: Dict[str, Any], year: int, quarter_int: int, top_k: int) -> Dict[str, Any]:
    rows = list_company_metrics_by_name(company_row.get("display_name"), year, quarter_int)
    table = []
    for r in rows:
        cur = r.get("metric_value"); yoyb = r.get("last_year_value"); qoqb = r.get("last_period_value")
        yoy = None if (cur is None or yoyb is None) else cur - yoyb
        qoq = None if (cur is None or qoqb is None) else cur - qoqb
        table.append({"metric": r.get("metric_name"),
                      "current": cur, "yoy_change": yoy, "qoq_change": qoq,
                      "current_str": fmt_num(cur), "yoy_change_str": fmt_num(yoy), "qoq_change_str": fmt_num(qoq)})
    def key_yoy(x): v = x.get("yoy_change"); return abs(v) if isinstance(v,(int,float)) else -1
    def key_qoq(x): v = x.get("qoq_change"); return abs(v) if isinstance(v,(int,float)) else -1
    top_yoy = sorted(table, key=key_yoy, reverse=True)[:max(1, top_k)]
    top_qoq = sorted(table, key=key_qoq, reverse=True)[:max(1, top_k)]
    return {"type":"anomaly","title":f"å¼‚åŠ¨åˆ†æï¼ˆTOP{top_k}ï¼‰","top_yoy":top_yoy,"top_qoq":top_qoq}

@app.post("/deepanalysis/analyze", response_model=AnalyzeResp)
def analyze(req: AnalyzeReq, _=Depends(require_token)):
    # åŒæ­¥ç‰ˆï¼šæ”¶é›†è¿›åº¦åä¸€æ¬¡æ€§è¿”å›ï¼ˆä¸ä½ åŸæœ‰è¡Œä¸ºä¸€è‡´ï¼‰
    return _analyze_core(req, on_push=None)

# === æ–°å¢ï¼šSSE æµå¼æ¥å£ ===
@app.post("/deepanalysis/analyze/stream")
# deepanalysis_agent.py -> analyze_stream()
async def analyze_stream(req: AnalyzeReq, _=Depends(require_token)):
    async def event_gen():
        q: asyncio.Queue = asyncio.Queue()

        def on_push(ev: Dict[str, Any]):
            try: q.put_nowait(("progress", ev))
            except Exception: pass

        task = asyncio.create_task(asyncio.to_thread(_analyze_core, req, on_push))

        while True:
            if task.done():
                # â† åœ¨çœŸæ­£å‘é€æœ€ç»ˆç»“æœä¹‹å‰ï¼Œç­‰è‡³å°‘ THOUGHT_DELAY_MS
                await asyncio.sleep(max(THOUGHT_DELAY_MS, 500)/1000.0)
                try:
                    resp: AnalyzeResp = task.result()
                    payload = json.dumps(resp.dict(), ensure_ascii=False)
                    yield f"event: done\ndata:{payload}\n\n"
                except Exception as e:
                    yield f"event: done\ndata:{json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"
                break

            try:
                typ, ev = await asyncio.wait_for(q.get(), timeout=0.1)
                yield f"event: {typ}\ndata:{json.dumps(ev, ensure_ascii=False)}\n\n"
            except asyncio.TimeoutError:
                continue


    return StreamingResponse(event_gen(), media_type="text/event-stream")




if __name__ == "__main__":
    import uvicorn
    uvicorn.run("deepanalysis_agent:app", host="0.0.0.0", port=18030, reload=False)
